import rclpy
import os
import time
from rclpy.node import Node
import pyaudio
from playsound import playsound
import wave
import threading
import webrtcvad
import queue
from std_msgs.msg import String,UInt16,Bool
from utils.mic_serial import kws_mic
from utils import large_model_interface
from utils.large_model_interface import rec_wav_music_en
from ament_index_python.packages import get_package_share_directory
class ASRNode(Node):
    def __init__(self):
        super().__init__('asr_node')
        # åˆå§‹åŒ–å‚æ•°ã€å˜é‡ / Initialize parameters and variables
        self.init_param_config()
        # åˆå§‹åŒ–è¯­éŸ³å”¤é†’ / Initialize keyword spotting (KWS)
        self.kws_init()
        # åˆå§‹åŒ–ASRæ¨¡åž‹ / Initialize ASR model
        self.asr_mdoel_init()
        # åˆå§‹åŒ–è¯­è¨€è®¾ç½® / Initialize language settings
        self.language_init()
        # åˆå§‹åŒ–ç³»ç»Ÿå£°éŸ³ / Initialize system sound functionality
        self.system_sound_init()
        # åˆå§‹åŒ–ROSé€šä¿¡ / Initialize ROS communication
        self.init_ros_comunication()
        # æ‰“å°åˆå§‹åŒ–ä¿¡æ¯ / Log initialization completion
        self.get_logger().info("asr_node Initialization completed")

    def init_ros_comunication(self):
        # åˆ›å»ºèœ‚é¸£å™¨å‘å¸ƒè€… / Create a publisher for the buzzer
        self.pub_beep = self.create_publisher(UInt16, "beep", 10)
        # åˆ›å»ºASRå‘å¸ƒè€…ï¼Œå‘å¸ƒè½¬æ¢å®Œæˆçš„æ¶ˆæ¯ / Create an ASR publisher to publish conversion results
        self.asr_pub = self.create_publisher(String, "asr", 5)
        # åˆ›å»ºå”¤é†’ä¿¡æ¯å‘å¸ƒè€… / Create a publisher for wake-up signals
        self.wakeup_pub = self.create_publisher(Bool, "wakeup", 5)

    def init_param_config(self):
        self.user_speechdir = os.path.join(get_package_share_directory('largemodel'), "resources_file", "user_speech.wav")
        # å‚æ•°å£°æ˜Ž / Declare parameters
        self.declare_parameter('VAD_MODE', 1)
        self.declare_parameter('sample_rate', 16000) 
        self.declare_parameter('frame_duration_ms', 30)        
        self.declare_parameter('language', 'en')
        self.declare_parameter('use_oline_asr', False) 
        self.declare_parameter('mic_serial_port', "/dev/ttyUSB1")
        self.declare_parameter('mic_index', 0)
        self.declare_parameter("regional_setting", "China")

        # èŽ·å–æœåŠ¡å™¨å‚æ•° / Get server parameters
        self.VAD_MODE = self.get_parameter('VAD_MODE').get_parameter_value().integer_value
        self.sample_rate = self.get_parameter('sample_rate').get_parameter_value().integer_value
        self.frame_duration_ms = self.get_parameter('frame_duration_ms').get_parameter_value().integer_value
        self.language = self.get_parameter('language').get_parameter_value().string_value  
        self.use_oline_asr = self.get_parameter('use_oline_asr').get_parameter_value().bool_value
        self.mic_serial_port=self.get_parameter('mic_serial_port').get_parameter_value().string_value
        self.mic_index=self.get_parameter('mic_index').get_parameter_value().integer_value
        self.regional_setting = (self.get_parameter("regional_setting").get_parameter_value().string_value)

        self.frame_bytes = int(self.sample_rate * self.frame_duration_ms / 1000) # éŸ³é¢‘å¸§å¤§å° / Audio frame size

        # å¤§æ¨¡åž‹æŽ¥å£å®žä¾‹ç«¯ / Instance of the large model interface
        self.modelinterface=large_model_interface.model_interface()
        # åˆå§‹åŒ– WebRTC VAD / Initialize WebRTC VAD
        self.vad = webrtcvad.Vad()
        self.vad.set_mode(self.VAD_MODE)
        self.current_thread = None  # å”¤é†’å¤„ç†çº¿ç¨‹ / Thread for handling wake-up events
        self.stop_event=threading.Event()

    def main_loop(self):
        while rclpy.ok():
            while self.audio_request_queue.qsize() > 1:  # åªå¤„ç†æœ€è¿‘çš„ä¸€æ¬¡å”¤é†’è¯·æ±‚ï¼Œé˜²æ­¢é‡å¤å”¤é†’ / Process only the most recent wake-up request to prevent duplicates
                self.audio_request_queue.get()
            if not self.audio_request_queue.empty():
                self.audio_request_queue.get()
                self.wakeup_pub.publish(Bool(data=True))  # å‘å¸ƒå”¤é†’ä¿¡å· / Publish wake-up signal
                self.get_logger().info("I'm here")
                playsound(self.audio_dict[self.first_response])  # åº”ç­”ç”¨æˆ· / Respond to the user

                if self.current_thread and self.current_thread.is_alive():  # æ‰“æ–­ä¸Šæ¬¡çš„å”¤é†’å¤„ç†çº¿ç¨‹ / Interrupt the previous wake-up handling thread
                    self.stop_event.set()
                    self.current_thread.join()  # ç­‰å¾…å½“å‰çº¿ç¨‹ç»“æŸ / Wait for the current thread to finish
                    self.stop_event.clear()  # æ¸…é™¤äº‹ä»¶ / Clear the event
                self.current_thread = threading.Thread(target=self.kws_handler)
                self.current_thread.daemon = True
                self.current_thread.start()            
            rclpy.spin_once(self, timeout_sec=0.1)
            time.sleep(0.1)

    def kws_handler(self)->None:
        if self.stop_event.is_set():
            return      

        if self.listen_for_speech(self.mic_index):
            asr_text = self.ASR_conversion(self.user_speechdir)  # è¿›è¡Œ ASR è½¬æ¢ / Perform ASR conversion
            if asr_text =='error':  # æ£€æŸ¥ ASR ç»“æžœé•¿åº¦æ˜¯å¦å°äºŽ4ä¸ªå­—ç¬¦ / Check if ASR result length is less than 4 characters
                self.get_logger().warn("I still don't understand what you mean. Please try again")
                playsound(self.audio_dict[self.error_response])  # é”™è¯¯å“åº” / Error response
            else:
                self.get_logger().info(asr_text)
                self.get_logger().info("okayðŸ˜€, let me think for a moment...")
                self.asr_pub_result(asr_text)  # å‘å¸ƒ ASRç»“æžœ / Publish ASR result            
        else:
            return

    def system_sound_init(self):  # åˆå§‹åŒ–ç³»ç»Ÿå£°éŸ³ç›¸å…³çš„åŠŸèƒ½ / Initialize system sound functionality
        pkg_path=get_package_share_directory('largemodel')
        self.audio_dict={}  # ç³»ç»Ÿå£°éŸ³å­—å…¸ / Dictionary of system sounds
        self.audio_dict['longwan-women-1']=os.path.join(pkg_path, "resources_file", "longwan-women-1.mp3")
        self.audio_dict['longwan-women-2']=os.path.join(pkg_path, "resources_file", "longwan-women-2.mp3")
        self.audio_dict['longxiaochun-women-1']=os.path.join(pkg_path, "resources_file", "longxiaochun-women-1.mp3")
        self.audio_dict['longxiaochun-women-2']=os.path.join(pkg_path, "resources_file", "longxiaochun-women-2.mp3")

    def asr_mdoel_init(self):  # åˆå§‹åŒ–asræ¨¡åž‹ / Initialize ASR model
        if self.regional_setting == "international": 
            if self.use_oline_asr:
                self.get_logger().info(f"The online asr model :XUN-FEI ASR is loaded")   
            else:
                self.modelinterface.init_local_asr_model()
                self.get_logger().info(f"The local asr model :XUN-FEI ASR is loaded")
            
        elif self.regional_setting == "China":
            if self.use_oline_asr:
                # -------- SenseVoiceSmall è¯­éŸ³è¯†åˆ«  --æ¨¡åž‹åŠ è½½----- / Load SenseVoiceSmall online ASR model
                self.get_logger().info(f'The online asr model :{self.modelinterface.init_oline_asr(self.language)} is loaded')
            else:
                self.modelinterface.init_local_asr_model()
                self.get_logger().info('The asr model :SenseVoiceSmall is loaded')
        else:
            while True:
                self.get_logger().info('Please check the regional_setting parameter in yahboom.yaml file, it should be either "China" or "international".')
                time.sleep(1)

    def language_init(self):
        if self.language=='zh':
            self.first_response='longwan-women-1'
            self.error_response='longwan-women-2'        
        elif self.language=='en':
            self.first_response='longxiaochun-women-1'
            self.error_response='longxiaochun-women-2'
        else:
            while True:  
                self.get_logger().error("Language setting error, please check your language setting")
                time.sleep(3)

    def kws_init(self):  # åˆå§‹åŒ–å…³é”®è¯å”¤é†’ç›¸å…³çš„å†…å®¹ / Initialize keyword spotting (KWS) related content
        self.port_name =self.mic_serial_port
        self.audio_request_queue = queue.Queue()  # ç”¨äºŽä¼ é€’éŸ³é¢‘è¯·æ±‚ / Queue for passing audio requests
        self.serial_port = kws_mic(port=self.port_name, kwsquence=self.audio_request_queue,baudrate=115200)
        self.serial_port.open()
        if not self.serial_port.ser or not self.serial_port.ser.is_open:
            self.get_logger().error("Failed to open KWS serial port.")
        receive_thread = threading.Thread(target=self.serial_port.receive_data)
        receive_thread.daemon = True
        receive_thread.start()                 

    def asr_pub_result(self,asr_result:str)->None:
        msg=String(data=asr_result)
        self.asr_pub.publish(msg)

    def ASR_conversion(self, input_file:str)->str:
        if self.regional_setting == "international":  
            if self.use_oline_asr:
                res=rec_wav_music_en()
                if res is not None:
                    return res
                else:
                    return "error"
            else:
                result=self.modelinterface.SenseVoiceSmall_ASR(input_file)
                if result[0] == 'ok' and len(result[1]) > 4:
                    return result[1]
                else:
                    self.get_logger().error(f'ASR Error:{result[1]}')  # ASR error.
                    return 'error'
        else:
            if self.use_oline_asr:
                result=self.modelinterface.oline_asr(input_file)
                if result[0] == 'ok' and len(result[1]) > 4:
                    return result[1]
                else:
                    self.get_logger().error(f'ASR Error:{result[1]}')  # ASR error.
                    return 'error'
            else:
                result=self.modelinterface.SenseVoiceSmall_ASR(input_file)
                if result[0] == 'ok' and len(result[1]) > 4:
                    return result[1]
                else:
                    self.get_logger().error(f'ASR Error:{result[1]}')  # ASR error.
                    return 'error'            

    def listen_for_speech(self,mic_index=0):
        p = pyaudio.PyAudio()   # Create PyAudio instance. / åˆ›å»ºPyAudioå®žä¾‹ã€‚
        audio_buffer = []       # Store audio data. / å­˜å‚¨éŸ³é¢‘æ•°æ®ã€‚
        silence_counter = 0     # Silence counter. / é™éŸ³è®¡æ•°å™¨ã€‚
        MAX_SILENCE_FRAMES = 90  # 30å¸§*30ms=900msé™éŸ³åŽåœæ­¢ / Stop after 900ms of silence (30 frames * 30ms)
        speaking = False  # Flag indicating speech activity. / è¯­éŸ³æ´»åŠ¨æ ‡å¿—ã€‚
        frame_counter = 0  # Frame counter. / è®¡æ•°å™¨ã€‚
        stream_kwargs = {
            'format': pyaudio.paInt16,
            'channels': 1,
            'rate': self.sample_rate,
            'input': True,
            'frames_per_buffer': self.frame_bytes,
        }
        if mic_index != 0:
            stream_kwargs['input_device_index'] = mic_index

        # Prompt the user to speak via the buzzer. / é€šè¿‡èœ‚é¸£å™¨æç¤ºç”¨æˆ·è®²è¯ã€‚
        self.pub_beep.publish(UInt16(data = 1))
        time.sleep(0.5)
        self.pub_beep.publish(UInt16(data = 0))

        try:
            # Open audio stream. / æ‰“å¼€éŸ³é¢‘æµã€‚
            stream = p.open(**stream_kwargs)
            while True:
                if self.stop_event.is_set():
                    return False 
        
                frame = stream.read(self.frame_bytes, exception_on_overflow=False)  # Read audio data. / è¯»å–éŸ³é¢‘æ•°æ®ã€‚
                is_speech = self.vad.is_speech(frame, self.sample_rate)  # VAD detection. / VADæ£€æµ‹ã€‚
  
                if is_speech:
                    # Detected speech activity. / æ£€æµ‹åˆ°è¯­éŸ³æ´»åŠ¨ã€‚
                    speaking = True
                    audio_buffer.append(frame)
                    silence_counter = 0
                else:
                    if speaking:
                        # Detect silence after speech activity. / åœ¨è¯­éŸ³æ´»åŠ¨åŽæ£€æµ‹é™éŸ³ã€‚
                        silence_counter += 1
                        audio_buffer.append(frame)  # Continue recording buffer. / æŒç»­è®°å½•ç¼“å†²ã€‚
                        
                        # End recording when silence duration meets the threshold. / é™éŸ³æŒç»­æ—¶é—´è¾¾æ ‡æ—¶ç»“æŸå½•éŸ³ã€‚
                        if silence_counter >= MAX_SILENCE_FRAMES:
                            break
                frame_counter += 1
                if frame_counter % 2 == 0:
                    self.get_logger().info('1' if is_speech else '-')
                    # Real-time status display.
        finally:
            stream.stop_stream()
            stream.close()
            p.terminate()

        # Save valid recording (remove trailing silence). / ä¿å­˜æœ‰æ•ˆå½•éŸ³ï¼ˆåŽ»é™¤å°¾éƒ¨é™éŸ³ï¼‰ã€‚
        if speaking and len(audio_buffer) > 0:
            # Trim the last silent part. / è£å‰ªæœ€åŽé™éŸ³éƒ¨åˆ†ã€‚
            clean_buffer = audio_buffer[:-MAX_SILENCE_FRAMES] if len(audio_buffer) > MAX_SILENCE_FRAMES else audio_buffer
            
            with wave.open(self.user_speechdir, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))
                wf.setframerate(self.sample_rate)
                wf.writeframes(b''.join(clean_buffer))
                return True



def main(args=None):
    rclpy.init(args=args)
    sense_voice_node = ASRNode()
    try:
        sense_voice_node.main_loop()
    except KeyboardInterrupt:
        pass
    finally:
        sense_voice_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()


